{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQt6KdrzysRw"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hr7A6HA4y0J_",
    "outputId": "f77b91c0-713a-4738-f441-5cc1c6611dde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvxoeRgYzKY1"
   },
   "outputs": [],
   "source": [
    "#!unzip  \"/gdrive/MyDrive/CaseStudy_HomeCredit/home-credit-default-risk.zip\"  -d \"/gdrive/MyDrive/CaseStudy_HomeCredit/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7_xWQwpysR6"
   },
   "source": [
    "<pre>\n",
    "Serious complication for loan providers(Banking) is to find out the Loan applicants who are very likely to repay the loan.If the loan provider predicted the faithful customer, they will gain more profit and  more imporatanly they will avoid losses.\n",
    "In kaggle, Home Credit Default Risk is currently using various statistical and machine learning methods to make these predictions, they're challenging. Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n",
    "\n",
    "Here we have two classes in the target label, we will conclude this problem as a <strong> Binary classification </strong> problem.\n",
    "\n",
    "<strong>Problem Statement</strong>\n",
    "We need to Build the model,which will say by giving the customer information whether this applicant will repay a loan or not\n",
    " \n",
    "<strong>Data Description</strong>\n",
    "They are 7 different source of table for this problem, combining all the tables is 221 features.\n",
    "two main tables are <strong>application_train.csv</strong> and <strong>application_test.csv</strong>, which contain the current applications of the clients who have applied for loan. All the other tables are referenced with this table using the unique ID, i.e. SK_ID_CURR.\n",
    "For more information, visit - https://www.kaggle.com/c/home-credit-default-risk/data\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PcSL6EYnysR-"
   },
   "outputs": [],
   "source": [
    "# Select the directory \n",
    "os.chdir('/gdrive/My Drive/CaseStudy_HomeCredit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6SGqtvFysTc"
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "MsEDg7a8Km0o",
    "outputId": "4abf22ae-a12f-4053-c3ae-4d8f64613a52"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing Useful DataStructures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#importing Misc Libraries\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "\n",
    "#sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "jKdy8hm2Kmnw"
   },
   "outputs": [],
   "source": [
    "def missing_value( dataframe = 'df', df_name ='TRAIN', visualizse = True, head_count = 5):\n",
    "    '''\n",
    "    dataframe - Pass the dataframe to the find missing value of each row.\n",
    "    df_name  -  Dataframe name\n",
    "    Visualizse - Boolean, True- Visalizse or False - skip the visualizse part\n",
    "    head_count - #print top five missing column in descending column.\n",
    "    '''\n",
    "    total = dataframe.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (dataframe.isnull().sum()/dataframe.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    #print(missing_data.head(head_count))\n",
    "    \n",
    "    if visualizse:\n",
    "        # visualizse\n",
    "        figure(figsize=(50,50), facecolor='w', edgecolor='r')\n",
    "        '''\n",
    "        edge color - Border color\n",
    "        dpi - dots-per-inch\n",
    "        '''\n",
    "        if len(missing_data.index) >20 :\n",
    "            sns.set(font_scale = 3)\n",
    "            sns.barplot(y= missing_data.index, x= missing_data['Percent'])\n",
    "        else: \n",
    "            sns.set(font_scale = 6)\n",
    "            sns.barplot(y= missing_data.index, x= missing_data['Percent'])\n",
    "        plt.tight_layout()\n",
    "        plt.title(str(df_name))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def reduce_mem_usage(data, verbose = True):\n",
    "    # refer - https://medium.com/@aakashgoel12/avoid-memory-error-techniques-to-reduce-dataframe-memory-usage-fcf53b2318a2\n",
    "    #refer: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "    '''\n",
    "    This function is used to reduce the memory usage by converting the datatypes of a pandas\n",
    "    DataFrame withing required limits.\n",
    "    '''\n",
    "    \n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('-'*100)\n",
    "        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "        print('-'*100)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Read all madatory files for this task\n",
    "def read_mandatory_files(test_data='test_dataframe'):\n",
    "    #df = pd.read_csv('application_train.csv')\n",
    "    #df = reduce_mem_usage(df)\n",
    "    #df_test = pd.read_csv('application_test.csv')\n",
    "    #df_test = reduce_mem_usage(df_test)\n",
    "    #print(type(test_data))\n",
    "    df_test = test_data\n",
    "    df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "    df_bureau = pd.read_csv('bureau.csv')\n",
    "    df_bureau = reduce_mem_usage(df_bureau)\n",
    "    df_bureau_bal = pd.read_csv('bureau_balance.csv')\n",
    "    df_bureau_bal = reduce_mem_usage(df_bureau_bal)\n",
    "    df_prev_app = pd.read_csv('previous_application.csv')\n",
    "    df_prev_app = reduce_mem_usage(df_prev_app)\n",
    "    df_pos = pd.read_csv('POS_CASH_balance.csv')\n",
    "    df_pos = reduce_mem_usage(df_pos)\n",
    "    df_credit_bal = pd.read_csv('credit_card_balance.csv')\n",
    "    df_credit_bal = reduce_mem_usage(df_credit_bal)\n",
    "    df_inst_pay = pd.read_csv('installments_payments.csv')\n",
    "    df_inst_pay = reduce_mem_usage(df_inst_pay)\n",
    "    \n",
    "    return df_test, df_bureau,df_bureau_bal,df_prev_app,df_pos, df_credit_bal,df_inst_pay\n",
    "\n",
    "\n",
    "\n",
    "# Data Cleaning and preprocessing for train and test\n",
    "def data_cleaning(df='dataframe'):\n",
    "    df['DAYS_BIRTH'] = round(df['DAYS_BIRTH'] *-1/ 365)\n",
    "\n",
    "    # abs - convert all value in to postive\n",
    "    df['DAYS_EMPLOYED']  = abs(df['DAYS_EMPLOYED'])\n",
    "    df['DAYS_EMPLOYED'].head(2)\n",
    "\n",
    "    # Replace the anomalous values(Errorness value) with nan\n",
    "    df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace({365243: np.nan})\n",
    "\n",
    "    # Convert days in to years\n",
    "    df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'] /365\n",
    "    \n",
    "    # Create an anomalous flag column\n",
    "    df['Year_Empolyed_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "    #Invalid Gender code, we have limited entry so we removing\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "\n",
    "    df.loc[df['OBS_30_CNT_SOCIAL_CIRCLE'] > 30, 'OBS_30_CNT_SOCIAL_CIRCLE'] = np.nan\n",
    "    df.loc[df['OBS_60_CNT_SOCIAL_CIRCLE'] > 30, 'OBS_60_CNT_SOCIAL_CIRCLE'] = np.nan\n",
    "\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# replacing Null with XNS -  Unknow category\n",
    "def replacing_missing_category(df = 'dataframe'):\n",
    "    # replacing Null with XNS -  Unknow category\n",
    "    categorical_columns_train = df.dtypes[df.dtypes == 'object'].index.tolist()\n",
    "    df[categorical_columns_train] = df[categorical_columns_train].fillna('XNA')\n",
    "    \n",
    "    # From EDA REGION_RATING_CLIENT and REGION_RATING_CLIENT_W_CITY have discret value, \n",
    "    #so we changing this column data type from Int to Object\n",
    "    \n",
    "    df['REGION_RATING_CLIENT'] = df['REGION_RATING_CLIENT'].astype('object')\n",
    "    df['REGION_RATING_CLIENT_W_CITY'] = df['REGION_RATING_CLIENT_W_CITY'].astype('object')\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "## Converting Category in to Numerical representation using Onehot encoder and label encoder\n",
    "'''\n",
    "OneHotEncoder -  handle_unknown='ignore'\n",
    "    unknown category is encountered during\n",
    "    transform, the resulting one-hot encoded columns for this feature\n",
    "    will be all zeros.\n",
    "'''\n",
    "def category_to_numeric(df_test = 'dataframe2'):\n",
    "    #print('Shape of application test before Encoding ', df_test.shape)\n",
    "    cat_df = df_test.dtypes[df_test.dtypes == 'object'].index.tolist()\n",
    "    #enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    for i in tqdm(cat_df):\n",
    "        #enc = pickle.load('Encoder//'+str(i)+'_onehot.pkl') \n",
    "        with open(('Encoder//'+str(i)+'_onehot.pkl') , 'rb') as f:\n",
    "          enc = pickle.load(f)\n",
    "        enc_df_test = pd.DataFrame(enc.transform(df_test[[i]]).toarray().astype('int'),columns=enc.get_feature_names([i]))\n",
    "        df_test = df_test.drop(i,axis=1).join(enc_df_test)\n",
    "\n",
    "    #print('Shape of application test after Encoding ', df_test.shape)  \n",
    "    \n",
    "    return df_test\n",
    "\n",
    "# replacing ,missing value with Median\n",
    "def replacing_missing_numeric(df = 'dataframe', df_test='dataframe2'):\n",
    "\n",
    "    # Test missing values\n",
    "    missing_value(df_test,df_name='TEST',visualizse=False,head_count=10)\n",
    "    \n",
    "    ## Fill in missing values\n",
    "\n",
    "    #Strategy = Median, variances is high so better to use Median\n",
    "    imputer = Imputer(strategy = 'median')\n",
    "    scaler = MinMaxScaler(feature_range = [0,1])\n",
    "    train = df_test\n",
    "    train_col = train.columns\n",
    "\n",
    "\n",
    "    with open((r'Imputer_folder/_Imputer.pkl') , 'rb') as f:\n",
    "      imputer = pickle.load(f)\n",
    "      \n",
    "    test = imputer.transform(df_test)\n",
    "    with open((r'Scalar/_ScalarImputer.pkl') , 'rb') as f:\n",
    "      scaler = pickle.load(f)    \n",
    "    test = scaler.transform(test)\n",
    "    #print('Testing data shape: ', test.shape)\n",
    "    \n",
    "    new_df_test = pd.DataFrame(test,columns=train_col)\n",
    "    new_df_test['SK_ID_CURR'] = df_test['SK_ID_CURR'].values\n",
    "    \n",
    "    # Test missing values\n",
    "    missing_value(new_df_test,df_name='TEST',visualizse=False,head_count=5)\n",
    "\n",
    "    #print(' Observation : \\n 1.Now there is no missing value in Train and test')\n",
    "    \n",
    "    \n",
    "    return  new_df_test\n",
    "\n",
    "\n",
    "\n",
    "def create_custom_features_main_table( df_test='dataframe2'):\n",
    "\n",
    "\n",
    "    ################ Application_ test.csv ###########\n",
    "\n",
    "    df_test['INCOME_GT_CREDIT_FLAG'] = df_test['AMT_INCOME_TOTAL'] > df_test['AMT_CREDIT']\n",
    "    df_test['DIR'] = df_test['AMT_CREDIT']/(df_test['AMT_INCOME_TOTAL']+ 1)\n",
    "    df_test['AIR'] = df_test['AMT_ANNUITY']/(df_test['AMT_INCOME_TOTAL'] +1)\n",
    "    df_test['ACR'] = df_test['AMT_CREDIT']/(df_test['AMT_CREDIT']+1)\n",
    "    df_test['DAR'] = df_test['DAYS_EMPLOYED']/df_test['DAYS_BIRTH']\n",
    "    \n",
    "    return df_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# when merging two table, chance o having same column name in both table, to avoid we settiing new feature name\n",
    "# Note , pd.Dummies we have prefix attribute, but for numerical field better to use this function\n",
    "def create_unique_col(table= 'Bureau',data='df', ID ='SK_ID_CURR' ):\n",
    "    '''\n",
    "    table - Dataframe name\n",
    "    data  = Dataframe\n",
    "    ID    = Foreign key\n",
    "    return Column name wit prefix table name\n",
    "    '''\n",
    "    \n",
    "    unique_col_bureau = [] \n",
    "    for i in data.columns:\n",
    "        if i != ID:\n",
    "            col_name = table+str('_')+str(i)\n",
    "            unique_col_bureau.append(col_name)\n",
    "        else:\n",
    "            unique_col_bureau.append(i)\n",
    "    #print('New Column names  - \\n'+str(unique_col_bureau))   \n",
    "    return unique_col_bureau  \n",
    "\n",
    "\n",
    "\n",
    "def create_custom_bureau_feature(df_bureau='bureau_table'):\n",
    "    # Create a new column , using existing information from Bureau\n",
    "    # Number of past loans per customer\n",
    "    past_loan = df_bureau.groupby(by = ['SK_ID_CURR'])['SK_ID_BUREAU'].count().reset_index().rename(columns = {'SK_ID_BUREAU': 'LOAN_COUNT_BUREAU'})\n",
    "    #print('Past loan details',past_loan.shape )\n",
    "\n",
    "\n",
    "    # Number of type of credit loan type per customer\n",
    "    credit_type = df_bureau[['SK_ID_CURR', 'CREDIT_TYPE']].groupby('SK_ID_CURR')['CREDIT_TYPE'].nunique().reset_index().rename(columns={'CREDIT_TYPE': 'LOAN_TYPES_BUREAU'})\n",
    "    #print('credit_type details',credit_type.shape)\n",
    "\n",
    "\n",
    "    # total_loan amount still date\n",
    "    sum_total_count = df_bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM']].groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM': 'TOTAL_LOAN_AMT_SUM'})\n",
    "    #print('sum_total_count details',sum_total_count.shape )\n",
    "\n",
    "\n",
    "    # total_loan amount debt still date\n",
    "    sum_total_count_debt = df_bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM_DEBT']].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_DEBT': 'TOTAL_LOAN_AMT_SUM_DEBT'})\n",
    "    #print('sum_total_count_debt details',sum_total_count_debt.shape)\n",
    "\n",
    "\n",
    "    #merge sum_total_count and sum_total_count_debt\n",
    "    debt_credit_df  =  sum_total_count.merge(sum_total_count_debt,on='SK_ID_CURR')\n",
    "    debt_credit_df['debt_credit_ratio'] = debt_credit_df['TOTAL_LOAN_AMT_SUM_DEBT'] / (debt_credit_df['TOTAL_LOAN_AMT_SUM'] +1)\n",
    "    #print('Merge of sum_total_count and sum_total_count_debt',debt_credit_df.shape)\n",
    "\n",
    "\n",
    "    # Sum of AMT_CREDIT_SUM_OVERDUE\n",
    "    Total_customer_overdue  = df_bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM_OVERDUE']].groupby('SK_ID_CURR')['AMT_CREDIT_SUM_OVERDUE'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_OVERDUE': 'SUM_AMT_CREDIT_SUM_OVERDUE'})\n",
    "    #print('Sum of over due amount',Total_customer_overdue.shape)\n",
    "    # sum AMT_CREDIT_SUM_DEBT\n",
    "    Total_customer_debt =  df_bureau[['SK_ID_CURR','AMT_CREDIT_SUM_DEBT']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_DEBT':'TOTAL_CUSTOMER_DEBT'})\n",
    "    #print('Sum of total debt amount ',Total_customer_debt.shape)\n",
    "\n",
    "\n",
    "    sum_overdue_credit_sum  = Total_customer_overdue.merge(Total_customer_debt,on='SK_ID_CURR')\n",
    "    sum_overdue_credit_sum['overdue_debt_ratio'] = sum_overdue_credit_sum['SUM_AMT_CREDIT_SUM_OVERDUE'] / (sum_overdue_credit_sum['TOTAL_CUSTOMER_DEBT']+1)\n",
    "    sum_overdue_credit_sum['overdue_debt_ratio'] = sum_overdue_credit_sum['overdue_debt_ratio'].fillna(0)\n",
    "    sum_overdue_credit_sum['overdue_debt_ratio'] = sum_overdue_credit_sum.replace([np.inf,-np.inf],0)\n",
    "    sum_overdue_credit_sum['overdue_debt_ratio'] = pd.to_numeric(sum_overdue_credit_sum['overdue_debt_ratio'],downcast='float')\n",
    "    #print('Ratio of Overdue and credit debt amount',sum_overdue_credit_sum.shape)\n",
    "    \n",
    "    df_bureau = df_bureau.merge(past_loan,on='SK_ID_CURR',how='left')\n",
    "    df_bureau =df_bureau.merge(credit_type,on='SK_ID_CURR',how='left')\n",
    "    df_bureau =df_bureau.merge(debt_credit_df,on='SK_ID_CURR',how='left')\n",
    "    df_bureau =df_bureau.merge(sum_overdue_credit_sum,on='SK_ID_CURR',how='left')\n",
    "    \n",
    "    return df_bureau\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Joining application_train and BUREAU.csv\n",
    "def join_application_bureau(df ='table1', df_bureau = 'table2'):\n",
    "    # Categorical feature - merging\n",
    "    # Converting all categorical in to onehot encoding\n",
    "    categorical_bureau = pd.get_dummies(df_bureau.select_dtypes('object'), prefix='Bureau')\n",
    "    categorical_bureau['SK_ID_CURR'] = df_bureau['SK_ID_CURR']\n",
    "\n",
    "    grp_bureau = categorical_bureau.groupby(by = ['SK_ID_CURR']).mean().reset_index()\n",
    "    #print('Column_names_Categorical', grp_bureau.columns)\n",
    "    \n",
    "    # Merge train and bureau_categorical\n",
    "    df_main = df.merge(grp_bureau, on='SK_ID_CURR',how='left')\n",
    "    df_main.update(df_main[grp_bureau.columns].fillna(0))\n",
    "    \n",
    "    # Combining Numerical features\n",
    "\n",
    "    Numerical_bureau_col = df_bureau.select_dtypes(include=[np.number]).columns\n",
    "    Numerical_bureau = df_bureau[Numerical_bureau_col]\n",
    "\n",
    "\n",
    "    grp_bureau_num = Numerical_bureau.drop(['SK_ID_BUREAU'], axis = 1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
    "    #print('Column_names_Numeric', grp_bureau_num.columns)\n",
    "\n",
    "    #There may be some column name for both application and bureau.csv, inorder to avoid we giving unique column name\n",
    "    grp_bureau_num.columns = create_unique_col(table='Bureau',data=grp_bureau_num,ID='SK_ID_CURR')\n",
    "\n",
    "    # Merge train and bureau_categorical\n",
    "    df_main = df_main.merge(grp_bureau_num, on='SK_ID_CURR',how='left')\n",
    "    df_main.update(df_main[grp_bureau_num.columns].fillna(0))\n",
    "    \n",
    "    return df_main\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Joining Bureau_Balance data to merge of Application and Bureau(df_main)\n",
    "\n",
    "def data_cleaning_bureau_bal(df_bureau_bal= 'dataframe'):\n",
    "    # C - Close , so we giving weight to 0, remaining incremental, thing is X - Unknow so benefit of doubt we giving middle value(4) \n",
    "    status_label_encoding = { 'C': 0, '0': 1, '1': 2, '2': 3, 'X': 4, '3': 5, '4': 6, '5': 7}\n",
    "    df_bureau_bal['STATUS'] = df_bureau_bal['STATUS'].map(status_label_encoding)\n",
    "\n",
    "    # Monthly Balance is in Negative , easy interpreatation we changing to postive\n",
    "    df_bureau_bal['MONTHS_BALANCE'] = abs(df_bureau_bal['MONTHS_BALANCE'])\n",
    "\n",
    "    # Creating new features 'Weightage_balance' = divide Status by Months_balance \n",
    "    df_bureau_bal['WEIGHT_status'] = df_bureau_bal['STATUS'] / (df_bureau_bal['MONTHS_BALANCE'] +1)\n",
    "    \n",
    "    return df_bureau_bal\n",
    "\n",
    "def merge_application_BureauBal(df_main='dataframe1', df_bureau='dataframe2', df_bureau_bal='dataframe3'):\n",
    "    df_bureau_bal = data_cleaning_bureau_bal(df_bureau_bal= df_bureau_bal)\n",
    "    Bureau_merge_Bureau_bal =  df_bureau.merge(df_bureau_bal, on='SK_ID_BUREAU')\n",
    "    Bureau_merge_Bureau_bal = Bureau_merge_Bureau_bal[['SK_ID_CURR', 'MONTHS_BALANCE','STATUS','WEIGHT_status']].groupby('SK_ID_CURR')['MONTHS_BALANCE','STATUS','WEIGHT_status'].sum().reset_index()\n",
    "    Bureau_merge_Bureau_bal.columns = create_unique_col(table='Bureau_bal',data=Bureau_merge_Bureau_bal,ID='SK_ID_CURR')\n",
    "    df_main = df_main.merge(Bureau_merge_Bureau_bal, on='SK_ID_CURR',how='left')\n",
    "    df_main.update(df_main[['Bureau_bal_MONTHS_BALANCE','Bureau_bal_STATUS','Bureau_bal_WEIGHT_status']].fillna(0))\n",
    "    #print('Shape of main table after merge Application, Bureau and Bureau_balance', df_main.shape)\n",
    "    \n",
    "    return df_main\n",
    "\n",
    "\n",
    "\n",
    "def create_custom_prevapp_feature(previous_application='bureau_table'):\n",
    "    # Create a new column , using existing information from Previous applications\n",
    "    #https://www.kaggle.com/c/home-credit-default-risk/discussion/64598\n",
    "    previous_application['AMT_INTEREST'] = previous_application['CNT_PAYMENT'] * previous_application[\n",
    "                                            'AMT_ANNUITY'] - previous_application['AMT_CREDIT'] \n",
    "    previous_application['INTEREST_SHARE'] = previous_application['AMT_INTEREST'] / (previous_application[\n",
    "                                                                                            'AMT_CREDIT'] + 0.00001)\n",
    "    previous_application['INTEREST_RATE'] = 2 * 12 * previous_application['AMT_INTEREST'] / (previous_application[\n",
    "                                        'AMT_CREDIT'] * (previous_application['CNT_PAYMENT'] + 1))\n",
    "    \n",
    "\n",
    "    previous_application['AMT_DECLINED'] = previous_application['AMT_APPLICATION'] - previous_application['AMT_CREDIT']\n",
    "\n",
    "    previous_application['AMT_CREDIT_GOODS_RATIO'] = previous_application['AMT_CREDIT'] / (previous_application['AMT_GOODS_PRICE'] + 0.00001)\n",
    "    previous_application['AMT_CREDIT_GOODS_DIFF'] = previous_application['AMT_CREDIT'] - previous_application['AMT_GOODS_PRICE']\n",
    "\n",
    "    previous_application['ANNUITY'] = previous_application['AMT_CREDIT'] / (previous_application['CNT_PAYMENT'] + 0.00001)\n",
    "    previous_application['ANNUITY_GOODS'] = previous_application['AMT_GOODS_PRICE'] / (previous_application['CNT_PAYMENT'] + 0.00001)\n",
    "   \n",
    "    #print('After creating custom feature ', previous_application.shape)\n",
    "    return previous_application\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_application_prev_app(df_main = 'dataframe1',df_prev_app ='dataframe2'):\n",
    "    Pre_app_count= df_prev_app[['SK_ID_CURR','SK_ID_PREV']].groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].count().reset_index().rename(columns={'SK_ID_PREV':'PREV_APP_COUNT'}).fillna(0)\n",
    "\n",
    "    # Combining categorical features\n",
    "    pre_app_categorical = pd.get_dummies(df_prev_app.select_dtypes('object'))\n",
    "    pre_app_categorical['SK_ID_CURR'] = df_prev_app['SK_ID_CURR']\n",
    "\n",
    "    grp_PrevApp = pre_app_categorical.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    grp_PrevApp.columns = create_unique_col(table='PREV_APP',data=grp_PrevApp,ID='SK_ID_CURR')\n",
    "\n",
    "    # Combine final Previous_Application to df_main\n",
    "    df_main = df_main.merge(grp_PrevApp,on='SK_ID_CURR',how='left')\n",
    "    df_main.update(df_main[grp_PrevApp.columns].fillna(0))\n",
    "\n",
    "    # Combining numerical features\n",
    "    grp_PrevApp_numeric = df_prev_app.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
    "    grp_PrevApp_numeric.columns = create_unique_col(table='PREV_APP',data=grp_PrevApp_numeric,ID='SK_ID_CURR')\n",
    "    df_main = df_main.merge(grp_PrevApp_numeric, on =['SK_ID_CURR'], how = 'left')\n",
    "    df_main.update(df_main[grp_PrevApp_numeric.columns].fillna(0))\n",
    "\n",
    "    #print('Shape after merge Application, Bureau, Bureau_Balance and Previous_Application ', df_main.shape)\n",
    "    return df_main\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def create_custom_pos_feature(pos_cash='dataframe1'):\n",
    "    \n",
    "    #creating new features based on Domain Knowledge\n",
    "    pos_cash['SK_DPD_RATIO'] = pos_cash['SK_DPD'] / (pos_cash['SK_DPD_DEF'] + 0.00001)\n",
    "\n",
    "    pos_cash['TOTAL_TERM'] = pos_cash['CNT_INSTALMENT'] + pos_cash['CNT_INSTALMENT_FUTURE']\n",
    "\n",
    "    #print('Shape of POS after feature engineering', pos_cash.shape)\n",
    "\n",
    "    return pos_cash\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_application_pos(df_main = 'dataframe1',df_pos ='dataframe2'):\n",
    "    #Pre_app_count= df_prev_app[['SK_ID_CURR','SK_ID_PREV']].groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].count().reset_index().rename(columns={'SK_ID_PREV':'PREV_APP_COUNT'}).fillna(0)\n",
    "\n",
    "    POS_count= df_pos[['SK_ID_CURR','SK_ID_PREV']].groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].count().reset_index().rename(columns={'SK_ID_PREV':'POS_COUNT'}).fillna(0)\n",
    "\n",
    "    # Combining categorical features\n",
    "    POS_categorical = pd.get_dummies(df_pos.select_dtypes('object'))\n",
    "    POS_categorical['SK_ID_CURR'] = df_pos['SK_ID_CURR']\n",
    "\n",
    "    grp_POS = POS_categorical.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    grp_POS.columns = create_unique_col(table='POS',data=grp_POS,ID='SK_ID_CURR')\n",
    "\n",
    "    # Combine final Previous_Application to df_main\n",
    "    df_main = df_main.merge(grp_POS,on='SK_ID_CURR',how='left')\n",
    "    df_main.update(df_main[grp_POS.columns].fillna(0))\n",
    "\n",
    "    # Combining numerical features\n",
    "    POS_numeric = df_pos.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
    "    POS_numeric.columns = create_unique_col(table='POS',data=POS_numeric,ID='SK_ID_CURR')\n",
    "    df_main = df_main.merge(POS_numeric, on =['SK_ID_CURR'], how = 'left')\n",
    "    df_main.update(df_main[POS_numeric.columns].fillna(0))\n",
    "\n",
    "    #print('Shape after merge Application, Bureau, Bureau_Balance , Previous_Application and POS ', df_main.shape)\n",
    "    \n",
    "    return df_main\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def create_custom_Inspay_feature(installments_payments='bureau_table'):\n",
    "    # Create a new column , using existing information from Install payment applications\n",
    "    installments_payments['AMT_PAYMENT_DIFF'] = installments_payments['AMT_INSTALMENT'] - installments_payments['AMT_PAYMENT']\n",
    "    installments_payments['AMT_PAYMENT_RATIO'] = installments_payments['AMT_PAYMENT'] / (installments_payments['AMT_INSTALMENT'] + 0.00001)\n",
    "    installments_payments['DAYS_PAYMENT_RATIO'] = installments_payments['DAYS_INSTALMENT'] / (installments_payments['DAYS_ENTRY_PAYMENT'] + 0.00001)\n",
    "    installments_payments['DAYS_PAYMENT_DIFF'] = installments_payments['DAYS_INSTALMENT'] - installments_payments['DAYS_ENTRY_PAYMENT']\n",
    "\n",
    "    #print('Shape of InstallPayment after feature engineering', installments_payments.shape)\n",
    "\n",
    "    return installments_payments\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_application_Inspay(df_main = 'dataframe1',df_inst_pay ='dataframe2'):\n",
    "    INSPAY_numeric = df_inst_pay.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
    "    INSPAY_numeric.columns = create_unique_col(table='INSTPAY',data=INSPAY_numeric,ID='SK_ID_CURR')\n",
    "    df_main = df_main.merge(INSPAY_numeric, on =['SK_ID_CURR'], how = 'left')\n",
    "    df_main.update(df_main[INSPAY_numeric.columns].fillna(0))\n",
    "\n",
    "    #print('Shape after merge Application, Bureau, Bureau_Balance , Previous_Application, POS and InstantPay ', df_main.shape)\n",
    "    return df_main\n",
    "\n",
    "\n",
    "\n",
    "def create_custom_Credit_bal_feature(cc_balance='CCB_table'):\n",
    "\n",
    "     #Creating new features\n",
    "        cc_balance['AMT_DRAWING_SUM'] = cc_balance['AMT_DRAWINGS_ATM_CURRENT'] + cc_balance['AMT_DRAWINGS_CURRENT'] + cc_balance[\n",
    "                                    'AMT_DRAWINGS_OTHER_CURRENT'] + cc_balance['AMT_DRAWINGS_POS_CURRENT']\n",
    "        cc_balance['BALANCE_LIMIT_RATIO'] = cc_balance['AMT_BALANCE'] / (cc_balance['AMT_CREDIT_LIMIT_ACTUAL'] + 0.00001)\n",
    "        cc_balance['CNT_DRAWING_SUM'] = cc_balance['CNT_DRAWINGS_ATM_CURRENT'] + cc_balance['CNT_DRAWINGS_CURRENT'] + cc_balance[\n",
    "                                            'CNT_DRAWINGS_OTHER_CURRENT'] + cc_balance['CNT_DRAWINGS_POS_CURRENT'] + cc_balance['CNT_INSTALMENT_MATURE_CUM']\n",
    "        cc_balance['MIN_PAYMENT_RATIO'] = cc_balance['AMT_PAYMENT_CURRENT'] / (cc_balance['AMT_INST_MIN_REGULARITY'] + 0.0001)\n",
    "        cc_balance['PAYMENT_MIN_DIFF'] = cc_balance['AMT_PAYMENT_CURRENT'] - cc_balance['AMT_INST_MIN_REGULARITY']\n",
    "        cc_balance['MIN_PAYMENT_TOTAL_RATIO'] = cc_balance['AMT_PAYMENT_TOTAL_CURRENT'] / (cc_balance['AMT_INST_MIN_REGULARITY'] +0.00001)\n",
    "        cc_balance['PAYMENT_MIN_DIFF'] = cc_balance['AMT_PAYMENT_TOTAL_CURRENT'] - cc_balance['AMT_INST_MIN_REGULARITY']\n",
    "        cc_balance['AMT_INTEREST_RECEIVABLE'] = cc_balance['AMT_TOTAL_RECEIVABLE'] - cc_balance['AMT_RECEIVABLE_PRINCIPAL']\n",
    "        cc_balance['SK_DPD_RATIO'] = cc_balance['SK_DPD'] / (cc_balance['SK_DPD_DEF'] + 0.00001)\n",
    "\n",
    "        #print('Shape of Credit Card balance after feature engineering', cc_balance.shape)\n",
    "\n",
    "        return cc_balance\n",
    "\n",
    "\n",
    "\n",
    "def merge_application_credit_bal(df_main = 'dataframe1',df_credit_bal ='dataframe2'):\n",
    "    # Combining categorical features\n",
    "    CREBAL_categorical = pd.get_dummies(df_credit_bal.select_dtypes('object'))\n",
    "    CREBAL_categorical['SK_ID_CURR'] = df_credit_bal['SK_ID_CURR']\n",
    "\n",
    "    grp_CREDBAL = CREBAL_categorical.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    grp_CREDBAL.columns = create_unique_col(table='CREDITBAL',data=grp_CREDBAL,ID='SK_ID_CURR')\n",
    "\n",
    "    # Combine final Previous_Application to df_main\n",
    "    df_main = df_main.merge(grp_CREDBAL,on='SK_ID_CURR',how='left')\n",
    "    df_main.update(df_main[grp_CREDBAL.columns].fillna(0))\n",
    "\n",
    "    # Combining numerical features\n",
    "    CREDBAL_numeric = df_credit_bal.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\n",
    "    CREDBAL_numeric.columns = create_unique_col(table='CREDITBAL',data=CREDBAL_numeric,ID='SK_ID_CURR')\n",
    "    df_main = df_main.merge(CREDBAL_numeric, on =['SK_ID_CURR'], how = 'left')\n",
    "    df_main.update(df_main[CREDBAL_numeric.columns].fillna(0))\n",
    "\n",
    "    #print('Shape after merge Application, Bureau, Bureau_Balance , Previous_Application, POS, INSTALLMENT PAYMENT and CREDIt BAL ', df_main.shape)\n",
    "    \n",
    "    return df_main\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "yIhjg3QoduGw"
   },
   "outputs": [],
   "source": [
    "def final_function_1(input_data):\n",
    "    '''\n",
    "    Preprocessing the test dataframe -  input_data: DataFrame\n",
    "        The test datapoint, whose Target is to be predicted\n",
    "    '''\n",
    "    df_test, df_bureau,df_bureau_bal,df_prev_app,df_pos, df_credit_bal,df_inst_pay = read_mandatory_files(test_data=input_data)\n",
    "    print('Shape of Test1', df_test.shape)\n",
    "    # Data Cleaning\n",
    "    df_test =  data_cleaning(df = df_test)\n",
    "    print('Shape of Test2', df_test.shape)\n",
    "    \n",
    "    # Handling missing value-  Category\n",
    "    df_test =  replacing_missing_category(df = df_test)\n",
    "    print('Shape of Test3', df_test.shape)\n",
    "    # OneHot encoding and Label encoding\n",
    "    df_test =  category_to_numeric( df_test=df_test) \n",
    "    print('Shape of Test4', df_test.shape)\n",
    "    # create custome features for train and test application\n",
    "    df_test = create_custom_features_main_table(df_test=df_test)\n",
    "    print('Shape of application test ', df_test.shape)\n",
    "    # Handling missing value-  Category\n",
    "    df_test=  replacing_missing_numeric(df_test = df_test)\n",
    "    # Bureau table custom features\n",
    "    df_bureau = create_custom_bureau_feature(df_bureau=df_bureau)\n",
    "    # Join application and bureau table\n",
    "    df_main_test = join_application_bureau(df = df_test, df_bureau=df_bureau)\n",
    "    # Join application, bureau and Bureau_balance\n",
    "    df_main_test = merge_application_BureauBal(df_main=df_main_test, df_bureau=df_bureau, df_bureau_bal=df_bureau_bal)\n",
    "    #Creating custom features for previous applications\n",
    "    df_prev_app = create_custom_prevapp_feature(previous_application=df_prev_app)\n",
    "    # Join application, bureau , Bureau_balance and previous application\n",
    "    df_main_test = merge_application_prev_app(df_main=df_main_test, df_prev_app=df_prev_app)  #-------->\n",
    "    # create new feature based on existing column in POS table\n",
    "    df_pos = create_custom_pos_feature(pos_cash=df_pos)\n",
    "    # Join application, bureau , Bureau_balance , previous application and pos\n",
    "    df_main_test = merge_application_pos(df_main=df_main_test, df_pos=df_pos)  \n",
    "    #Creating custom features - Install payment features\n",
    "    df_inst_pay  = create_custom_Inspay_feature(installments_payments=df_inst_pay)\n",
    "    # Join application, bureau , Bureau_balance , previous application , pos and Installment payment\n",
    "    df_main_test = merge_application_Inspay(df_main=df_main_test, df_inst_pay=df_inst_pay)  \n",
    "    # Create new feature Creditcard balance\n",
    "    df_credit_bal = create_custom_Credit_bal_feature(cc_balance=df_credit_bal)\n",
    "    # Join application, bureau , Bureau_balance , previous application , pos , Installment payment and Credit balance\n",
    "    df_main_test = merge_application_credit_bal(df_main=df_main_test, df_credit_bal=df_credit_bal)  \n",
    "    \n",
    "    return df_main_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "jbZ1ZweZRDRa"
   },
   "outputs": [],
   "source": [
    "def final_function_2(main_preprocessor_data, y_test):\n",
    "      '''\n",
    "      Function 2 for prediction. This function takes both the Test Point and Target value of that point. It returns\n",
    "      the prediction along with the metric for the predicted points.\n",
    "                \n",
    "      '''\n",
    "      \n",
    "      start_time = datetime.now()\n",
    "      infile = open('select_features.txt','rb')\n",
    "      selected_features = pickle.load(infile)  \n",
    "      test_test = main_preprocessor_data[selected_features]\n",
    "\n",
    "      # load best model\n",
    "      # Saving the final model LightGBM as pickle file for the future use in productionizing the model\n",
    "      with open('Best_lgbm.pkl','rb') as fp:\n",
    "          best_model = pickle.load( fp)\n",
    "\n",
    "      y_pred_prob = best_model.predict(test_test)\n",
    "\n",
    "      y_pred = np.ones((len(test_test),), dtype=int)\n",
    "      for i in range(len(y_pred_prob)):\n",
    "          if y_pred_prob[i]<=0.5:\n",
    "              y_pred[i]=0\n",
    "          else:\n",
    "              y_pred[i]=1\n",
    "\n",
    "      ed = pd.DataFrame({'Actual':y_test[:test_test.shape[0]], 'Predicted': y_pred ,'prob':y_pred_prob})\n",
    "      pd.set_option('display.max_rows', None)\n",
    "      print(ed)\n",
    "\n",
    "      if len(test_test)>1:\n",
    "        print(\"Test Dataset Results:\")\n",
    "        print(f\"\\tROC-AUC Score = {roc_auc_score(y_test[:test_test.shape[0]], y_pred_prob)}\")\n",
    "        #print(f\"\\tPrecision Score = {precision_score(y_test[:test_test.shape[0]], y_pred)}\")\n",
    "        #print(f\"\\tRecall Score = {recall_score(y_test[:test_test.shape[0]], y_pred)}\")\n",
    "        #print(f\"\\tF1_Score  = {f1_score(y_test[:test_test.shape[0]], y_pred)}\")\n",
    "\n",
    "      else:\n",
    "\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Predicted Probabilties for given Client(s) being Defaulter is/are:\\n{np.round(y_pred_prob, 4)}\")\n",
    "        predicted_classes = np.where(y_pred_prob > 0.5, 1, 0)\n",
    "        print(f\"\\nThe predicted class labels are:\\n{predicted_classes}\")  \n",
    "\n",
    "      print(f\"Total Time taken for prediction = {datetime.now() - start_time}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "f-5Um4cndy-7"
   },
   "outputs": [],
   "source": [
    "# Read the Datatframe\n",
    "df_main = pd.read_csv('application_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "I6cem9sQTLVz",
    "outputId": "bb564697-0daa-44b2-de6a-453811bcbbc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the Test Query points for Testing Function 2 of pipeline is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE</th>\n",
       "      <th>NAME_INCOME_TYPE</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>NAME_HOUSING_TYPE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_REGISTRATION</th>\n",
       "      <th>DAYS_ID_PUBLISH</th>\n",
       "      <th>OWN_CAR_AGE</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_EMP_PHONE</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_CONT_MOBILE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>OCCUPATION_TYPE</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>WEEKDAY_APPR_PROCESS_START</th>\n",
       "      <th>HOUR_APPR_PROCESS_START</th>\n",
       "      <th>REG_REGION_NOT_LIVE_REGION</th>\n",
       "      <th>REG_REGION_NOT_WORK_REGION</th>\n",
       "      <th>LIVE_REGION_NOT_WORK_REGION</th>\n",
       "      <th>REG_CITY_NOT_LIVE_CITY</th>\n",
       "      <th>REG_CITY_NOT_WORK_CITY</th>\n",
       "      <th>LIVE_CITY_NOT_WORK_CITY</th>\n",
       "      <th>ORGANIZATION_TYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>LIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>LIVINGAREA_MEDI</th>\n",
       "      <th>NONLIVINGAPARTMENTS_MEDI</th>\n",
       "      <th>NONLIVINGAREA_MEDI</th>\n",
       "      <th>FONDKAPREMONT_MODE</th>\n",
       "      <th>HOUSETYPE_MODE</th>\n",
       "      <th>TOTALAREA_MODE</th>\n",
       "      <th>WALLSMATERIAL_MODE</th>\n",
       "      <th>EMERGENCYSTATE_MODE</th>\n",
       "      <th>OBS_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_30_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>OBS_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DEF_60_CNT_SOCIAL_CIRCLE</th>\n",
       "      <th>DAYS_LAST_PHONE_CHANGE</th>\n",
       "      <th>FLAG_DOCUMENT_2</th>\n",
       "      <th>FLAG_DOCUMENT_3</th>\n",
       "      <th>FLAG_DOCUMENT_4</th>\n",
       "      <th>FLAG_DOCUMENT_5</th>\n",
       "      <th>FLAG_DOCUMENT_6</th>\n",
       "      <th>FLAG_DOCUMENT_7</th>\n",
       "      <th>FLAG_DOCUMENT_8</th>\n",
       "      <th>FLAG_DOCUMENT_9</th>\n",
       "      <th>FLAG_DOCUMENT_10</th>\n",
       "      <th>FLAG_DOCUMENT_11</th>\n",
       "      <th>FLAG_DOCUMENT_12</th>\n",
       "      <th>FLAG_DOCUMENT_13</th>\n",
       "      <th>FLAG_DOCUMENT_14</th>\n",
       "      <th>FLAG_DOCUMENT_15</th>\n",
       "      <th>FLAG_DOCUMENT_16</th>\n",
       "      <th>FLAG_DOCUMENT_17</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136327</th>\n",
       "      <td>258126</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>12375.0</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.018029</td>\n",
       "      <td>-15398</td>\n",
       "      <td>-805</td>\n",
       "      <td>-6247.0</td>\n",
       "      <td>-5980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sales staff</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>FRIDAY</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Business Entity Type 3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-515.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180085</th>\n",
       "      <td>308693</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>1255500.0</td>\n",
       "      <td>36837.0</td>\n",
       "      <td>1255500.0</td>\n",
       "      <td>Family</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.035792</td>\n",
       "      <td>-16780</td>\n",
       "      <td>-2186</td>\n",
       "      <td>-5246.0</td>\n",
       "      <td>-328</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Drivers</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trade: type 7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1809.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86559</th>\n",
       "      <td>200451</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>792346.5</td>\n",
       "      <td>46458.0</td>\n",
       "      <td>684000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.006207</td>\n",
       "      <td>-10627</td>\n",
       "      <td>-3178</td>\n",
       "      <td>-5327.0</td>\n",
       "      <td>-3239</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SATURDAY</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Transport: type 2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1736</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>reg oper account</td>\n",
       "      <td>block of flats</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>Stone, brick</td>\n",
       "      <td>No</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1931.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188883</th>\n",
       "      <td>318979</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>242950.5</td>\n",
       "      <td>17806.5</td>\n",
       "      <td>184500.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Separated</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>-17037</td>\n",
       "      <td>-309</td>\n",
       "      <td>-9572.0</td>\n",
       "      <td>-510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Drivers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>MONDAY</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Transport: type 4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2653.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95768</th>\n",
       "      <td>211176</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>292500.0</td>\n",
       "      <td>1726866.0</td>\n",
       "      <td>47488.5</td>\n",
       "      <td>1543500.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>0.046220</td>\n",
       "      <td>-14023</td>\n",
       "      <td>-5463</td>\n",
       "      <td>-2954.0</td>\n",
       "      <td>-3310</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Managers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>TUESDAY</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Industry: type 11</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-872.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_CURR  ... AMT_REQ_CREDIT_BUREAU_YEAR\n",
       "136327      258126  ...                        1.0\n",
       "180085      308693  ...                        3.0\n",
       "86559       200451  ...                        0.0\n",
       "188883      318979  ...                        1.0\n",
       "95768       211176  ...                        NaN\n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Labels of These Datapoints are:\n",
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#pick the random 50 sample for testing\n",
    "test_datapoint_func_2 = df_main.sample(50).copy()\n",
    "targets_func_2 = test_datapoint_func_2.pop('TARGET')\n",
    "print(\"Some of the Test Query points for Testing Function 2 of pipeline is:\")\n",
    "display(test_datapoint_func_2.head(5))\n",
    "print(\"Target Labels of These Datapoints are:\")\n",
    "print(targets_func_2.values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBIgzMAIJFih",
    "outputId": "d86945cb-defa-4472-bd75-5d7300c02dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Memory usage of dataframe: 0.05 MB\n",
      "Memory usage after optimization: 0.02 MB\n",
      "Decreased by 67.2%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Memory usage of dataframe: 222.62 MB\n",
      "Memory usage after optimization: 112.95 MB\n",
      "Decreased by 49.3%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Memory usage of dataframe: 624.85 MB\n",
      "Memory usage after optimization: 338.46 MB\n",
      "Decreased by 45.8%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Memory usage of dataframe: 471.48 MB\n",
      "Memory usage after optimization: 309.01 MB\n",
      "Decreased by 34.5%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Memory usage of dataframe: 610.43 MB\n",
      "Memory usage after optimization: 238.45 MB\n",
      "Decreased by 60.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Memory usage of dataframe: 673.88 MB\n",
      "Memory usage after optimization: 289.33 MB\n",
      "Decreased by 57.1%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Memory usage of dataframe: 830.41 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 127.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization: 311.40 MB\n",
      "Decreased by 62.5%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Shape of Test1 (50, 121)\n",
      "Shape of Test2 (50, 122)\n",
      "Shape of Test3 (50, 122)\n",
      "Shape of Test4 (50, 255)\n",
      "Shape of application test  (50, 260)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main_preprocessor_data = final_function_1(input_data = test_datapoint_func_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIbPAuFYIux-",
    "outputId": "c233e75f-173d-4db6-b8f2-6fbae6332a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Actual  Predicted      prob\n",
      "136327       0          0  0.368356\n",
      "180085       0          0  0.165494\n",
      "86559        1          1  0.762132\n",
      "188883       0          0  0.390015\n",
      "95768        0          0  0.218944\n",
      "7898         0          0  0.138505\n",
      "53687        0          0  0.299007\n",
      "136654       0          0  0.394477\n",
      "258180       0          0  0.288464\n",
      "109496       0          0  0.304399\n",
      "235922       0          0  0.327238\n",
      "63739        0          1  0.621835\n",
      "159657       0          0  0.148817\n",
      "163790       0          0  0.431622\n",
      "301484       0          0  0.460465\n",
      "261726       0          0  0.279070\n",
      "242753       0          0  0.281217\n",
      "104681       1          0  0.454204\n",
      "32516        0          0  0.364331\n",
      "193314       1          1  0.597982\n",
      "87319        0          0  0.170689\n",
      "62962        0          1  0.745852\n",
      "128643       0          0  0.162236\n",
      "295758       0          0  0.284922\n",
      "113019       0          0  0.086707\n",
      "212075       0          1  0.572866\n",
      "120943       1          1  0.616623\n",
      "225150       0          0  0.370733\n",
      "183907       0          0  0.310422\n",
      "273650       0          1  0.662400\n",
      "79537        0          0  0.406438\n",
      "55875        0          0  0.401845\n",
      "222529       0          0  0.409269\n",
      "94192        0          0  0.309899\n",
      "266118       0          0  0.319900\n",
      "220863       0          0  0.421669\n",
      "151388       0          0  0.265538\n",
      "105149       0          0  0.305183\n",
      "245236       0          0  0.288694\n",
      "183022       0          0  0.077765\n",
      "185274       0          0  0.491374\n",
      "204329       0          0  0.191086\n",
      "53915        0          0  0.273565\n",
      "80088        0          1  0.533240\n",
      "216024       0          0  0.424528\n",
      "132581       0          0  0.179668\n",
      "27344        0          0  0.095995\n",
      "43698        0          0  0.099272\n",
      "45397        0          1  0.735926\n",
      "67628        0          0  0.258642\n",
      "Test Dataset Results:\n",
      "\tROC-AUC Score = 0.9130434782608696\n",
      "Total Time taken for prediction = 0:00:00.050162\n"
     ]
    }
   ],
   "source": [
    "final_function_2(main_preprocessor_data,targets_func_2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LIGH_GBM_BEST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
